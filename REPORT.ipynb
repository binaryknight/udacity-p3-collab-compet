{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This repository uses an implementation of the [Multi Agent Deep Deterministic Policy Gradients (MADDPG)](https://arxiv.org/abs/1706.02275) algorithm. The implementation heavily borrows from the benchmark implementation in the Deep Reinforcement Learning Nanodegree Program. \n",
    "\n",
    "The implementation is used to train 2 agents play tennis.  The details of the task can be found in [README.md](README.md) and the code can be executed using the [Collab_Compet_Submission.ipynb](Collab_Compet_Submission.ipynb) Jupyter notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Deep Deterministic Policy Gradients Algorithm\n",
    "\n",
    "## Elements\n",
    "\n",
    "### Actor\n",
    "* The MADDPG algorithm trains $n_a$ number of actors.\n",
    "* The state of the actor at time $t$ is denoted by $s_{t,i} \\in S_i$ and $i \\in \\{1,\\dots, n_a\\}$.\n",
    "* The actor's actions are denoted by $a_{t,i} \\in A_i$. \n",
    "* The actor computes actions using the function $\\mu_{i}:S_{i} \\mapsto A_{i}$.\n",
    "* In the MADDPG algorithm, the actor is approximated by the function $\\mu_{i}:S \\times \\Theta_{\\mu,i} \\mapsto A_{i}$ where\n",
    "   * $\\Theta_{\\mu,i}$ is set of parameters of the approximation, in this case the weights of the approximating neural network.\n",
    "* For each actor, the MADDPG algorithm uses 2 neural networks during training hence there are 2 sets of weights:\n",
    "   * $\\theta_{\\mu,i} \\in \\Theta_{\\mu,i}$: local neural network weights. These are updated at every training step using policy gradients.\n",
    "   * $\\hat{\\theta}_{\\mu,i} \\in \\Theta_{\\mu,i}$: target neural network weights. These are updated at a slower rate to improve the stability of training.\n",
    "* Each actor uses only the local neural network weights when determining actions in actual operation.   \n",
    "\n",
    "### Critic\n",
    "* For each actor, the critic approximates the Q function.\n",
    "* The MADDPG algorithm uses the state of the actor as well as additional signals to estimate the Q function. In this\n",
    "  implementation each critic uses the states of each actor, the actions of each actor and the state of the ball to estimate the Q function. \n",
    "* The use of these additional signals sets the MADDPG algorithm apart from the [DDPG](https://arxiv.org/abs/1509.02971)  algorithm where the critic uses only the state of the actor.\n",
    "* The observation vector at time $t$ is $o_{t,s}  =\\{s_{t,i}\\} \\in O_{s},\\; i \\in \\{1,\\dots, n_a\\}$.\n",
    "* The observation vector at time $t$ is $o_{t,a} =\\{a_{t,i}\\} \\in O_{a},\\; i \\in \\{1,\\dots, n_a\\}$.\n",
    "* For each actor, the critic is a function $Q_i:O_{s} \\times O_{a} \\times \\Theta_{q,i} \\mapsto \\mathbb{R}$ where\n",
    "  * $\\Theta_{q,i}$ is the set of parameters of the neural network used in the approximation.\n",
    "* For each actor the MADDPG algorithm uses 2 neural networks during training, hence there are 2 sets of parameters:\n",
    "  * $\\theta_{q,i} \\in \\Theta_q$ is the set of parameters used by the local neural network.\n",
    "  * $\\hat{\\theta}_{q,i} \\in \\Theta_q$ is the set of parameters used by the target neural network and are updated at a lower rate to improve the stability of the training. \n",
    "* Actors do not use the critic when determining actions during actual operation.\n",
    "\n",
    "### Noise Process and Randomizations in Actions\n",
    "* The MADDPG algorithm uses noise to induce exploration during training. The noise is generated by an [Ornstein - Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) denoted by $N_{i}(\\mu, \\theta,\\sigma)$ for each actor where \n",
    "  * $\\mu$ is the mean of the noise\n",
    "  * $\\sigma$ is the volatility\n",
    "  * $\\theta$ is the decay-rate.\n",
    "* For each actor the MADDPG algorithm computes the action values as follows\n",
    "  \\begin{align*}\n",
    "   a_{t,i} &= \\mu_{i}(s_{t,i}, \\theta_{\\mu,i}) + \\epsilon_t N_i(\\mu, \\theta, \\sigma)\\\\\n",
    "   e_{t+1} &= \\gamma_\\epsilon \\epsilon_t \n",
    "  \\end{align*}\n",
    "  where the randomness rate $\\epsilon_t$, is between [0,1], and $\\gamma_\\epsilon \\in (0,1)$ is the decay rate of $\\epsilon_t$ and the initial randomness rate is $\\epsilon_0$. \n",
    "* Each element of the action vector is clipped so that they are between -1 and 1:\n",
    "  \\begin{align*}\n",
    "   a_{t,i}[k] \\leftarrow \\max(\\min(1, a_{t,i}[k]), -1)\n",
    "  \\end{align*}\n",
    "\n",
    "### The Experience Replay Buffer \n",
    "* For each actor the MADDPG stores the experience $(s_{t, i}, a_{t,i}, r_{t+1,i}, s_{t+1,i},o_{t,s},o_{t,a},o_{t+1,s})$ where \n",
    "  * $s_{t,i}$ is the state at time $t$ of actor $i$,\n",
    "  * $s_{t+1,i}$ is the state at time $t+1$ of actor $i$, \n",
    "  * $a_{t+1,i}$ is the action at time $t$ of actor $i$,\n",
    "  * $r_{t+1,i}$ is the reward obtained at time $t+1$ of actor i,\n",
    "  * $o_{t,s}$ is the state observations at time $t$,\n",
    "  * $o_{t+1,s}$ is the state observations time $t+1$, \n",
    "  * $o_{t,a}$ is the action observations at time $t$,\n",
    "in a buffer of fixed length $N_B$.\n",
    "* The stored experiences are used in training.\n",
    "* The experiences are used in batches of size $N_s$.\n",
    "* The experiences are randomly sampled for each actor independently.\n",
    "\n",
    "### Critic and Actor Training\n",
    "* At every $T$ time steps, the local critic neural network is trained as follows:\n",
    "     1. For $K$ update steps repeat:\n",
    "        1. For each agent $i \\in \\{1,\\dots, n_a\\}$:    \n",
    "            1. Select a batch of size $N_s$ experiences randomly from the replay buffer.\n",
    "            2. For each experience , compute:\n",
    "               \\begin{align*}\n",
    "               \\hat{o}_{t+1,a,j} & = \\{\\mu_{k}(s_{t+1,k,j}, \\theta_{\\mu,k}),\\; k \\in 1,\\dots, n_a\\}\\;\\; \\forall j \\in \\{1, \\dots, N_s\\} \\\\\n",
    "               y_{i,j} &= r_{t+1,i,j} + \\gamma Q(o_{t+1,s,j}, \\hat{o}_{t+1,a,j},\\hat{\\theta}_{q,i})\\;\\; \\forall j \\in \\{1, \\dots, N_s\\}\n",
    "               \\end{align*}\n",
    "               where $\\gamma$ is the discount factor.\n",
    "            3. Update $\\theta_{q,i}$ as follows:\n",
    "               \\begin{align*}\n",
    "               \\theta_{q,i} &\\leftarrow \\theta_{q,i} + \\frac{\\alpha_q}{N_s} \\sum_{j=1}^{N_s}(y_{i,j}- Q(o_{t,s,j},o_{t,a,j}, \\theta_{q,i}))\\nabla_{\\theta_{q,i}} Q(o_{t,s,j},o_{t,a,j}, \\theta_{q,i})\n",
    "               \\end{align*}\n",
    "               to minimize the objective function\n",
    "               \\begin{align*}\n",
    "                L = \\frac{1}{N_s}\\sum_{j =1}^{N_s} (y_{i,j}- Q(o_{t,s,j},o_{t,a,j}, \\theta_{q,i}))^2 \n",
    "               \\end{align*} \n",
    "               where $\\alpha_q$ is the critic learning rate.       \n",
    "\n",
    "* For each actor,  the local actor neural network weights are updated using a policy gradient approach.\n",
    "* At every $T$ time steps, the local critic neural network is trained as follows:\n",
    "     1. For $K$ update steps repeat:\n",
    "        1. For each agent $i \\in \\{1,\\dots, n_a\\}$:\n",
    "            1. Select a batch of size $N_s$ experiences randomly from the replay buffer.\n",
    "            2. Compute the policy gradient and update the weights.\n",
    "            \\begin{align*}\n",
    "                \\nabla J_{\\theta_{\\mu,i}} &= \\frac{1}{N_s}\\sum_{j=1}^{N_s}\\nabla_{a_{i}} Q_{i}(o_{t,s,j},o_{t,a,j}, \\hat{\\theta}_{q,i})\\nabla_{\\theta_{\\mu,i}}\\mu(s_{t,i,j}, \\theta_{\\mu,i}) \\\\\n",
    "            \\theta_{\\mu,i} &\\leftarrow \\theta_{\\mu,i} + \\alpha_\\mu \\nabla J_{\\theta_{\\mu,i}}\n",
    "            \\end{align*}\n",
    "            where $\\alpha_\\mu$ is the learning rate for the actor. \n",
    "\n",
    "### Continuous Target Actor and Critic Update\n",
    "* For each agent, the target actor and critic neural network weights are updated gradually at every $T$ time steps and every time the local neural networks are as follows: \n",
    " \\begin{align*}\n",
    "   \\hat{\\theta}_{q,i} &\\leftarrow  (1-\\tau)\\hat{\\theta}_{q,i} + \\tau\\theta_{q,i} \\\\\n",
    "   \\hat{\\theta}_{\\mu,i} &\\leftarrow  (1-\\tau)\\hat{\\theta}_{\\mu,i} + \\tau\\theta_{\\mu,i}\n",
    "  \\end{align*}\n",
    "where $\\tau$ is the soft update parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Algorithm\n",
    "0. For each agent $i \\in 1,\\dots, n_a$\n",
    "   * Initialize $\\theta_{\\mu,i}$ and $\\theta_{q,i}$ randomly. \n",
    "   * Set $\\hat{\\theta}_{\\mu,i} = \\theta_{\\mu,i}$ and $\\hat{\\theta}_{q,i} = \\theta_{q,i}$. \n",
    "   * Set the parameters $\\tau$,$\\gamma$, $\\alpha_\\mu$, $\\alpha_q$, $N_B$, $N_s$, $\\mu$, $\\theta$, $\\sigma$, $\\epsilon_0$, $\\gamma_\\epsilon$, $T$, $K$, $M$ where $M$ is the maximum number of episodes.  \n",
    "   \n",
    "1. for $M$ episodes do:\n",
    "   1. For each actor initialize random process $N_{i}(\\mu,\\theta,\\sigma)$.\n",
    "   2. Receive initial states $\\{s_{1,i},\\; i \\in 1, \\dots, n_a\\}$ and initial state observation $\\{o_{1,s}\\}$.\n",
    "   3. Until episode is over do:\n",
    "      1. For each actor $i$, compute $\\{a_{t,i}\\}$ as  \n",
    "          \\begin{align*}\n",
    "            a_{t,i} &= \\mu(s_{t,i}, \\theta_{\\mu,i}) + \\epsilon_t N_{i}(\\mu, \\theta, \\sigma)\\\\\n",
    "            a_{t,i}[k] &\\leftarrow \\max(\\min(1, a_{t,i}[k]), -1)\\\\\n",
    "            e_{t+1}  &= \\gamma_\\epsilon \\epsilon_t \n",
    "          \\end{align*}\n",
    "      2. For each actor, execute $a_{t,i}$, receive $r_{t+1,i}$ and transition to new state $s_{t+1,i}$.\n",
    "      3. For each actor, store experience $\\{s_{t,i}, a_{t,i}, r_{t+1,i}, s_{t+1,i}, o_{t,s}, o_{t,a}, o_{t+1,s}\\}$ in the experience replay buffer.\n",
    "      4. If $t\\bmod T = 0$ and the number of experiences stored in the replay buffer is greater than $N_s$ then for each agent do:\n",
    "        1. for $u = 1,\\; K$ do\n",
    "            1. Obtain a batch of experiences from the replay buffer of size $N_s$.\n",
    "            2. Update the critic local network weights. \n",
    "                 * For each experience , compute:\n",
    "              \\begin{align*}\n",
    "                  \\hat{o}_{t+1,a,j} & = \\{\\mu_{k}(s_{t+1,k,j}, \\theta_{\\mu,k}),\\; k \\in 1,\\dots, n_a\\}\\;\\; \\forall j \\in \\{1, \\dots, N_s\\} \\\\\n",
    "               y_{i,j} &= r_{t+1,i,j} + \\gamma Q(o_{t+1,s,j}, \\hat{o}_{t+1,a,j},\\hat{\\theta}_{q,i})\\;\\; \\forall j \\in \\{1, \\dots, N_s\\}\n",
    "               \\end{align*}\n",
    "            3. Update $\\theta_{q,i}$ as follows:\n",
    "               \\begin{align*}\n",
    "               \\theta_{q,i} &\\leftarrow \\theta_{q,i} + \\frac{\\alpha_q}{N_s} \\sum_{j=1}^{N_s}(y_{i,j}- Q(o_{t,s,j},o_{t,a,j}, \\theta_{q,i}))\\nabla_{\\theta_{q,i}} Q(o_{t,s,j},o_{t,a,j}, \\theta_{q,i})\n",
    "               \\end{align*}\n",
    "                       \n",
    "            4. Update the actor local network weights using the policy gradient:   \n",
    "               \\begin{align*}\n",
    "                \\nabla J_{\\theta_{\\mu,i}} &= \\frac{1}{N_s}\\sum_{j=1}^{N_s}\\nabla_{a_{i}} Q_{i}(o_{t,s,j},o_{t,a,j}, \\hat{\\theta}_{q,i})\\nabla_{\\theta_{\\mu,i}}\\mu(s_{t,i,j}, \\theta_{\\mu,i}) \\\\\n",
    "               \\theta_{\\mu,i} &\\leftarrow \\theta_{\\mu,i} + \\alpha_\\mu \\nabla J_{\\theta_{\\mu,i}}\n",
    "               \\end{align*}\n",
    "            5. Soft update the critic target and actor target neural network weights.   \n",
    "                \\begin{align*}\n",
    "                   \\hat{\\theta}_{q,i} &\\leftarrow  (1-\\tau)\\hat{\\theta}_{q,i} + \\tau\\theta_{q,i} \\\\\n",
    "                   \\hat{\\theta}_{\\mu,i} &\\leftarrow  (1-\\tau)\\hat{\\theta}_{\\mu,i} + \\tau\\theta_{\\mu,i}\n",
    "                \\end{align*} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "## Neural Network Structure\n",
    "\n",
    "### Critic Training\n",
    "* When training the local critic neural networks, the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss)\n",
    "  is minimized instead of the mean squared error. The Huber loss is less sensitive to outliers. The desired performance criterion was attained with fewer episodes during training when the Huber loss was used.\n",
    "\n",
    "### Actor Target and Local Neural Networks\n",
    "\n",
    "* The structure is as follows for both the actor local and actor target neural networks:\n",
    "    1. Layer: Input Layer(input_dim = size of the state vector for each actor, output_dim = 512)\n",
    "    2. Normalization: Batch normalization(input_dim = 512, output_dim = 512)\n",
    "    3. Activation: Relu Activation\n",
    "    4. Layer: Hidden Layer(input_dim = 512, output_dim = 512)\n",
    "    5. Activation: Relu Activation\n",
    "    6. Layer: Hidden Layer(input_dim = 512, output_dim = 256)\n",
    "    7. Normalization: Batch normalization(input_dim = 512, output_dim = 256) \n",
    "    8. Layer: Output Layer(input_dim = 256, output_dim = action vector size (2))\n",
    "    9. Activation: Tanh\n",
    "    \n",
    "### Critic Target and Local Neural Networks\n",
    "\n",
    "* The structure is as follows for both the \n",
    "    1.  Layer: Input Layer (input_dim = size of the state observation vector,output_dim = 512)\n",
    "    2.  Normalization: Batch Normalization\n",
    "    3.  Activation: Relu\n",
    "    4.  Layer: Hidden (input_dim = 512 + size of the action observation vector, output_dim = 512)\n",
    "    5.  Activation: Relu\n",
    "    6.  Layer: Hidden Layer(input_dim = 512, output_dim = 512)\n",
    "    7.  Activation: Relu\n",
    "    8.  Layer: Hidden Layer(input_dim  = 512, output_dim = 256)\n",
    "    9.  Activation: Relu\n",
    "    10. Layer: Output Layer(input_dim  = 256, output_dim = 1) \n",
    "* Using a nework with more layers and more units per layer helped attain the desired performance criterion in fewer number of episodes. \n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "* Soft update parameter,$\\tau$: 0.001\n",
    "* Discount factor, $\\gamma$:0.99\n",
    "* Learning rate for the actor local training, $\\alpha_\\mu$:0.0001\n",
    "* Learning rate for the critic local training, $\\alpha_q$:0.0001\n",
    "* Replay buffer size, $N_B$: 1,000,000\n",
    "* Training batch size $N_s$: 256\n",
    "* Ornstein-Uhlenbeck mean, $\\mu$: 0.0\n",
    "* Ornstein-Uhlenbeck decay rate, $\\theta$:0.15\n",
    "* Ornstein-Uhlenbeck volatility, $\\sigma$:0.2\n",
    "* Initial randomness rate, $\\epsilon_0$: 1.0\n",
    "* Randomness rate decay rate, $\\gamma_\\epsilon$: 0.999\n",
    "* Frequency of update for the neural networks, $T$: 1\n",
    "* Number of steps taken at every update, $K$: 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training Results\n",
    " The agents achieve a moving average maximum score  of 0.5 or above in about 705 episodes. \n",
    "![Training Performance](training_performance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Directions\n",
    "* The hyper parameters for the algorithm have been chosen by trial and error. Better parameters can be found by using automated hyperparameter search techniques.\n",
    "* The performance of the agents from episode to episode varies a lot. The big question is how to ensure consistent performance between episodes. I tried increasing the neural network sizes, adding batch normalization and using Huber loss instead of mean squared error loss. All these helped with training times and achieving the desired training performance, but reduced the variability only slightly.\n",
    "* I observed that successful episodes are frequently followed by episodes where the performance is very unsatisfactory. I think this is because the experiences from low performance episodes might be dropped from the replay buffers. I think experience where the agents failed should be used more often in training. As a next step, I would like to implement [Prioritized experienced replay](https://arxiv.org/abs/1511.05952) where failures have higher priorities.\n",
    "* Finally I would like to experiment with  other algorithms in the context of multi agent reinforcement learning such as \n",
    "   * [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)\n",
    "   * [D4PG](https://openreview.net/pdf?id=SyZipzbCb):  This algorithm includes prioritized experience replay and is shown to achieve better performance quicker than DDPG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
